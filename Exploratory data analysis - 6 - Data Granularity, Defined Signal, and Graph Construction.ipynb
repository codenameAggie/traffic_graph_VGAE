{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import vincenty\n",
    "from sklearn import preprocessing\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import networkx as nx\n",
    "\n",
    "# First, import the data: show upto 100 columns\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "\n",
    "## Adding a month of data - Jan 2016 - Feb 2016 - download the data for more\n",
    "# list_of_df_names = ['fahrzeiten_soll_ist_20160103_20160109', 'fahrzeiten_soll_ist_20160110_20160116',\n",
    "#                     'fahrzeiten_soll_ist_20160117_20160123', 'fahrzeiten_soll_ist_20160124_20160130',\n",
    "#                     'fahrzeiten_soll_ist_20160131_20160206', 'fahrzeiten_soll_ist_20160207_20160213',\n",
    "#                     'fahrzeiten_soll_ist_20160214_20160220', 'fahrzeiten_soll_ist_20160221_20160227']\n",
    "\n",
    "# Just the first two weeks\n",
    "list_of_df_names = ['./travel_times_2016/fahrzeiten_soll_ist_20160103_20160109', './travel_times_2016/fahrzeiten_soll_ist_20160110_20160116']\n",
    "\n",
    "list_of_df = []\n",
    "for i in list_of_df_names:\n",
    "    list_of_df.append(pd.read_csv(i+'.csv'))\n",
    "df_1 = pd.concat(list_of_df, ignore_index=True)\n",
    "list_of_df.clear()\n",
    "\n",
    "\n",
    "####################\n",
    "df_haltepunkt = pd.read_csv(\"./travel_times_2016/haltepunkt.csv\")\n",
    "# Fix the gps\n",
    "# print(type(df_haltepunkt['GPS_Latitude'].apply(lambda x: float(x.replace(',', '.')))))\n",
    "# df_haltepunkt['GPS_Latitude'] = df_haltepunkt['GPS_Latitude'].apply(lambda x: float(x.replace(',', '.')))\n",
    "# df_haltepunkt['GPS_Longitude'] = df_haltepunkt['GPS_Longitude'].apply(lambda x: float(x.replace(',', '.')))\n",
    "\n",
    "\n",
    "# Bus Station\n",
    "df_haltestelle = pd.read_csv(\"./travel_times_2016/haltestelle.csv\")\n",
    "####################\n",
    "\n",
    "def find_dist(lat1, lon1, lat_2, lon2):\n",
    "    c1 = (lat1, lon1)\n",
    "    c2 = (lat_2, lon2)\n",
    "    return geopy.distance.vincenty(c1, c2).km\n",
    "####################\n",
    "# Some useful atts #\n",
    "####################\n",
    "\n",
    "# Target attributes - from: in seconds\n",
    "target_arrival_from = \"soll_an_von\"\n",
    "\n",
    "# Actual attributes - from: in seconds\n",
    "actual_arrival_from = \"ist_an_von\"\n",
    "\n",
    "# nominal departure from in seconds\n",
    "target_departure_from = \"soll_ab_von\"\n",
    "\n",
    "# actual  departure from in seconds\n",
    "actual_departure_from = \"ist_ab_von\"\n",
    "#######################################\n",
    "\n",
    "#######################################\n",
    "# Target attributes - from: in seconds\n",
    "target_arrival_to = \"soll_an_nach\"\n",
    "\n",
    "# Actual attributes - from: in seconds\n",
    "actual_arrival_to = \"ist_an_nach1\"\n",
    "\n",
    "# nominal departure from in seconds\n",
    "target_departure_to = \"soll_ab_nach\"\n",
    "\n",
    "# actual  departure from in seconds\n",
    "actual_departure_to = \"ist_ab_nach\"\n",
    "#####################################\n",
    "\n",
    "#####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not contain nans usually, but remove any just in case\n",
    "df_haltepunkt.dropna(axis=0, how='any', inplace=True, subset=['GPS_Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the gps\n",
    "df_haltepunkt['GPS_Latitude'] = df_haltepunkt['GPS_Latitude'].apply(lambda x: float(x.replace(',', '.')))\n",
    "df_haltepunkt['GPS_Longitude'] = df_haltepunkt['GPS_Longitude'].apply(lambda x: float(x.replace(',', '.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of records in df_1\n",
    "len(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the halt punkt from\n",
    "\n",
    "df_1['halt_punkt_id'] = df_1['halt_punkt_id_von']\n",
    "df_1 = df_1.merge(df_haltepunkt, on=['halt_punkt_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge punkt to\n",
    "\n",
    "df_1['halt_punkt_id'] = df_1['halt_punkt_id_nach']\n",
    "df_1 = df_1.merge(df_haltepunkt, on='halt_punkt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['halt_id'] = df_1['halt_id_x']\n",
    "df_1 = df_1.merge(df_haltestelle, on='halt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['halt_id'] = df_1['halt_id_y']\n",
    "df_1 = df_1.merge(df_haltestelle, on='halt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1.sort_values(by = ['betriebsdatum', 'soll_ab_von', 'fahrt_id', 'seq_von'], inplace=True)\n",
    "df_1 = df_1[df_1['betriebsdatum'] == df_1['datum_nach']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, the windowing is done; 18000 for starting point translates to 18000/60/60 which is 5 AM, and 86400 is 11:59 PM\n",
    "df_1 = df_1[(df_1['soll_an_nach'] > 18000) & (df_1['soll_an_nach'] < 86400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Signals\n",
    "target_stationary = 'target_stationary'\n",
    "target_travel_time = 'target_travel_time'\n",
    "delay = 'delay'\n",
    "\n",
    "actual_stationary = 'actual_stationary'\n",
    "actual_travel_time = 'actual_travel_time'\n",
    "\n",
    "df_1[target_stationary] = df_1[target_departure_from] - df_1[target_arrival_from]\n",
    "df_1[target_travel_time] = df_1[target_arrival_to] - df_1[target_departure_from]\n",
    "df_1[delay] = df_1[actual_arrival_from] - df_1[target_arrival_from]\n",
    "\n",
    "\n",
    "# Signal - 1 - actual arrival - actual departure\n",
    "df_1[actual_stationary] = df_1[actual_departure_from] - df_1[actual_arrival_from]\n",
    "df_1[actual_travel_time] = df_1[actual_arrival_to] - df_1[actual_departure_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create A Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pair of gps locations and calculating the signal at that time instance:\n",
    "\n",
    "# aggregating the gps locations of the same punkts\n",
    "df_new_x = df_1.groupby(['halt_id_x'], as_index=False)['GPS_Latitude_x', 'GPS_Longitude_x'].mean()\n",
    "df_new_y = df_1.groupby(['halt_id_y'], as_index=False)['GPS_Latitude_y', 'GPS_Longitude_y'].mean()\n",
    "# Merging the two frames signifying a road segment\n",
    "final = df_1.merge(df_new_x, on=['halt_id_x'])\n",
    "final = final.merge(df_new_y, on=['halt_id_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.rename(columns={'GPS_Latitude_x_y':'GPS_LAN_STELLE_FROM', 'GPS_Longitude_x_y':'GPS_LON_STELLE_FROM',\n",
    "                      'GPS_Latitude_y_y':'GPS_LAN_STELLE_TO', 'GPS_Longitude_y_y':'GPS_LON_STELLE_TO'}, inplace=True)\n",
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Road_Segment_From'] = list(zip(final.GPS_Latitude_x_x, final.GPS_Longitude_x_x))\n",
    "final['Road_Segment_To'] = list(zip(final.GPS_Latitude_y_x, final.GPS_Longitude_y_x))\n",
    "final['Road_Segment_Node'] = list(zip(final.Road_Segment_From, final.Road_Segment_To))\n",
    "final['Route_Node_id_tuple'] = list(zip(final.halt_punkt_id_von, final.halt_punkt_id_nach))\n",
    "\n",
    "\n",
    "### the above uses the punkts for the graph; uncomment below for stelles\n",
    "# final['Road_Segment_From'] = list(zip(final.GPS_LAN_STELLE_FROM, final.GPS_LON_STELLE_FROM))\n",
    "# final['Road_Segment_To'] = list(zip(final.GPS_LAN_STELLE_TO, final.GPS_LON_STELLE_TO))\n",
    "# final['Road_Segment_Node'] = list(zip(final.Road_Segment_From, final.Road_Segment_To))\n",
    "# final['Route_Node_id_tuple'] = list(zip(final.halt_id_x, final.halt_id_y))\n",
    "# # final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these columns to be easier on the memory\n",
    "# columns_to_drop = ['halt_diva_von', 'halt_punkt_diva_von', 'halt_diva_nach', 'halt_punkt_diva_nach', 'fw_no', 'fw_typ', 'fw_kurz']\n",
    "# final.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the graph:\n",
    "\n",
    "    Dealing with granularity issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new signal for the day of the week\n",
    "d = pd.to_datetime(final['betriebsdatum'], format='%d.%m.%y', dayfirst=True)\n",
    "# Monday == 0 … Sunday == 6\n",
    "final['day_of_week'] = [i.weekday() for i in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.fw_typ.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fw_typ 2 is dropped since it does not correspond to a tram or a bus\n",
    "final = final[final.fw_typ != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dist of fw_no\n",
    "final.fw_no.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(zip(final.GPS_Latitude_x_x, final.GPS_Longitude_x_x)))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode routes\n",
    "final['Route_Node_id_str'] = final.Route_Node_id_tuple.apply(lambda x: '{}_{}'.format(x[0], x[1]))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(final.Route_Node_id_str)\n",
    "final['Route_Node_id'] = le.transform(final.Route_Node_id_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[((final.day_of_week != 5) & (final.day_of_week != 6))]\n",
    "final = final[(final['soll_an_von'] >= 21600) & (final['soll_an_von'] <= 72000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.fw_no.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(final.Road_Segment_Node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two kinds of signal\n",
    "final['actual_travel_time_n_target'] = final.actual_travel_time.div(final.target_travel_time, axis=0)\n",
    "final['actual_travel_time_n_target_mean'] = final.actual_travel_time.div(final.target_travel_time.mean(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, define the time period; 600 represents 10 mins (600/60)\n",
    "time_period = 600\n",
    "# Previously was at 5 am\n",
    "starting_time = 21600 # starting time is set at 21600 to maximize graph nodes\n",
    "# indicates 10 mins intervals\n",
    "# Number of bins:\n",
    "n_bins = int(np.floor((72000 - starting_time)/time_period))\n",
    "labels = [i for i in range(n_bins)]\n",
    "# Finding the correct bins:\n",
    "list_of_bins = [starting_time + time_period * i for i in range(n_bins + 1)]\n",
    "final['intervals'] = pd.cut(final[target_arrival_from], bins=list_of_bins, retbins=False, labels=labels, right=True, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.Route_Node_id.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final['actual_travel_time_n_target'] = final.actual_travel_time / final.target_travel_time\n",
    "# final.actual_travel_time_n_target = final.actual_travel_time_n_target - final.actual_travel_time_n_target.mean()\n",
    "# final.actual_travel_time_n_target_mean = final.actual_travel_time_n_target_mean - final.actual_travel_time_n_target_mean.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = final[final.Route_Node_id == 1158    ]\n",
    "a = a[a.betriebsdatum == '05.01.16']\n",
    "# a = a.sort_values(by=target_departure_from)\n",
    "a = a.groupby(by='intervals', as_index=False).mean()\n",
    "# a.intervals.value_counts()\n",
    "# a = a.groupby(by='intervals', as_index=False).mean()\n",
    "# a.fillna(value=0)\n",
    "# plt.plot(a[target_departure_from], a.target_travel_time)\n",
    "plt.plot(a.intervals.values.tolist(), a.actual_travel_time_n_target.values.tolist(), a.intervals.values.tolist(), a.actual_travel_time_n_target_mean.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = final[final.Route_Node_id == 1158]\n",
    "a = a[a.betriebsdatum == '05.01.16']\n",
    "for i in set(a.fahrweg_id):\n",
    "    b = a[a.fahrweg_id == i]\n",
    "    b.sort_values(by=['intervals', target_departure_from], inplace=True)\n",
    "    if b.shape[0] < 50:\n",
    "        continue\n",
    "    plt.plot(b[target_departure_from], b.actual_travel_time_n_target)\n",
    "a = a.groupby(by='intervals', as_index=False).mean()\n",
    "plt.title('Signal')\n",
    "plt.xlabel('Times of the day')\n",
    "plt.ylabel('')\n",
    "a.sort_values(by=['intervals', target_departure_from], inplace=True)\n",
    "plt.scatter([time_period * i + 21600 for i in a.intervals.values.tolist()], np.array(a.actual_travel_time_n_target.values.tolist()), marker='^', s=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.Route_Node_id.value_counts()[580:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Nodes with gran. problems:\n",
    "# when specifying some of the parameters above, some nodes may have to be dropped to ensure that each node has signal values\n",
    "# for that specific time stamp\n",
    "\n",
    "list_road_seg = final.Route_Node_id.value_counts()\n",
    "list_road_seg_id = []\n",
    "list_empty_bins_index = []\n",
    "list_to_drop = []\n",
    "\n",
    "for index, item in list_road_seg.items():\n",
    "    temp = final[final.Route_Node_id == index]\n",
    "    if (0 in (temp.intervals.value_counts().value_counts())):\n",
    "        list_road_seg_id.append(index)\n",
    "        list_empty_bins_index.append(temp.intervals.value_counts().value_counts()[0])\n",
    "        list_to_drop.append(index)\n",
    "    else:\n",
    "        list_road_seg_id.append(index)\n",
    "        list_empty_bins_index.append(0)\n",
    "list_empty_bins_index = np.array(list_empty_bins_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(final.Route_Node_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the number of missing values\n",
    "len(list_empty_bins_index)\n",
    "pd.Series(list_empty_bins_index).hist()\n",
    "plt.title('Distribution of empty and non empty bins - 10 mins - Start at 7 - 10')\n",
    "plt.xlabel('Road ID')\n",
    "plt.ylabel('Number of empty bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib\n",
    "# Plot the halts vs. empty bins\n",
    "plt.scatter(list_road_seg_id, list_empty_bins_index)\n",
    "plt.title('Scatter of empty and non empty bins - 10 mins  - Start at 7 - 10')\n",
    "plt.xlabel('Road ID')\n",
    "plt.ylabel('Number of empty bins')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nodes are to be kept if one allows for up to 5 interpolated points, vs. no interpolation\n",
    "print(np.count_nonzero(list_empty_bins_index < 6))\n",
    "print(np.count_nonzero(list_empty_bins_index == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Clear Issue with granularity\n",
    "solving it by limiting the start date and visualizing the distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rev_dist_bins = pd.Series(index = final.intervals.value_counts().values, data=final.intervals.value_counts().index.values)\n",
    "plt.scatter(final.intervals.value_counts().index.values, final.intervals.value_counts().values)\n",
    "plt.title('scatter - bin number vs data count - start at 7, end at 12 - 10 mins')\n",
    "plt.xlabel('Bin number')\n",
    "plt.ylabel('Count of Data points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.intervals.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(final.intervals.value_counts().index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop those datapoints:\n",
    "print(np.count_nonzero(list_empty_bins_index != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list_empty_bins_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[~final.Route_Node_id.isin(list_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final.Road_Segment_Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.Route_Node_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define new signal: Normalized travel time - close to TTI - Version 1, normalize by target travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['actual_travel_time_n_target'] = final.actual_travel_time.div(final.target_travel_time, axis=0)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define new signal: Normalized travel time - Version 2, normalize by mean of target in that interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['actual_travel_time_n_target_mean'] = final.actual_travel_time.div(final.target_travel_time.mean(), axis=0)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Signal First!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Signals for all of the nodes:\n",
    "List_of_Nodes = [str(i) for i in list(set(final.Route_Node_id))]\n",
    "# List_of_Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(List_of_Nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which signal to use? actual_travel_time_n_target or actual_travel_time_n_target_mean\n",
    "\n",
    "List_of_head_data = {}\n",
    "for i in List_of_Nodes:\n",
    "    temp_df = final[final.Route_Node_id == int(i)]\n",
    "    time_series_for_node = []\n",
    "    for j in sorted(list(set(temp_df.betriebsdatum)), key=lambda x: (x[3], x[4], x[0], x[1])):\n",
    "        time_series = temp_df[temp_df.betriebsdatum == j]\n",
    "        time_series = time_series.groupby(by='intervals', as_index=False).mean()\n",
    "        time_series = time_series['actual_travel_time_n_target']\n",
    "        time_series_for_node.append(time_series)\n",
    "    final_series = pd.concat(time_series_for_node, ignore_index=True)\n",
    "    List_of_head_data[i] = final_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = List_of_head_data['184']\n",
    "new_dict = {}\n",
    "for key, value in List_of_head_data.items():\n",
    "    new_dict[key] = value.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding dicts with distribution\n",
    "nan_dist = pd.Series(new_dict)\n",
    "nan_dist.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(final_data_df.columns))\n",
    "final_data_df = pd.DataFrame.from_dict(List_of_head_data)\n",
    "# print(sorted(final_data_df.isna().sum()))\n",
    "final_data_df.dropna(thresh=len(final_data_df) - 15, axis=1, inplace=True)\n",
    "# print(len(final_data_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(final_data_df.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_df = final_data_df.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 726     656\n",
    "# 769     581\n",
    "sig_1 = final_data_df['24']\n",
    "sig_2 = final_data_df['23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_data_df.index, sig_1, final_data_df.index, sig_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(sig_1,sig_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal is Defined, Create the graphs in two ways! Using the signal DF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The graph - V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Mid_point_GPS_data'] = final.Road_Segment_Node.apply(lambda x: (((x[0][0] + x[1][0])/2) , ((x[0][1] + x[1][1])/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_data_df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nodes_Road = list(set(final_data_df.columns.values.tolist()))\n",
    "# list_of_nodes = list(set(zip(final.Route_Node_id, final.Mid_point_GPS_data)))\n",
    "(list_of_nodes_Road)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import vincenty\n",
    "from geopy.distance import geodesic\n",
    "from networkx.drawing.nx_pydot import write_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nodes_Road_toKeep = [int(i) for i in list_of_nodes_Road]\n",
    "final = final[final.Route_Node_id.isin(list_of_nodes_Road_toKeep)]\n",
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List of dicts:\n",
    "new_final = final.groupby(by='Route_Node_id', as_index=False).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_List = pd.Series(new_final.Mid_point_GPS_data.values,index=new_final.Route_Node_id).to_dict()\n",
    "len(Dict_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find edges and weights:\n",
    "list_of_edges = []\n",
    "\n",
    "# iterate over each mid point aka road segment\n",
    "\n",
    "for i in list_of_nodes_Road:\n",
    "    # Look at every other road segment, if physical distance was less than 2.5 kilometers, add an edge\n",
    "    \n",
    "    for key, value in Dict_List.items():\n",
    "        if Dict_List[int(i)] != value:\n",
    "            if vincenty(Dict_List[int(i)], value).meters < 1500:\n",
    "                list_of_edges.append((int(i), key))\n",
    "    \n",
    "#     a = new_final[new_final.Route_Node_id == int(i)]\n",
    "#     a.Route_Node_id_tuple.iloc[5]\n",
    "#     for j in list_of_nodes:\n",
    "#         if i[0] != j[0]:\n",
    "#             if vincenty(i[1], j[1]).meters < 1000:\n",
    "#                 list_of_edges.append((i[0], j[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zlst = list(zip(*list_of_edges))\n",
    "edge_series = pd.Series(zlst[1], index = zlst[0])\n",
    "edge_series.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(list_of_nodes_Road_toKeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(list_of_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edges_from(list_of_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Graph\n",
    "nx.draw_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [int(i) for i in final_data_df.columns.values.tolist()]\n",
    "\n",
    "adj_mat = nx.to_numpy_matrix(G, nodelist=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following data is needed for the next notebook for time evolution map\n",
    "dict_gps = pd.Series(final.Mid_point_GPS_data.values,index=final.Route_Node_id).to_dict()\n",
    "# print(len(dict_gps))\n",
    "for i, v in dict_gps.items():\n",
    "    if i not in list_of_nodes_Road_toKeep:\n",
    "        del dict_gps[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(dict_gps)\n",
    "d.to_csv('gps_10min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_df.to_csv('final_10min.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig1 = final[final.Route_Node_id == 740]\n",
    "sig1 = sig1[sig1.betriebsdatum == '07.01.16']\n",
    "sig1.sort_values(by='soll_an_von', inplace=True)\n",
    "# sig1 = sig1\n",
    "sig1 = sig1.groupby(by='intervals', as_index=False).mean()\n",
    "# sig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig2 = final[final.Route_Node_id == 454]\n",
    "sig2 = sig2[sig2.betriebsdatum == '07.01.16']\n",
    "sig2.sort_values(by='soll_an_von', inplace=True)\n",
    "sig2 = sig2.groupby(by='intervals', as_index=False).mean()\n",
    "# sig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig1_X = np.array(sig1.intervals)\n",
    "sig1_y = np.array(sig1.actual_travel_time_n_target)\n",
    "sig1_y_zeromean = sig1_y - sig1_y.mean()\n",
    "\n",
    "sig2_X = np.array(sig2.intervals)\n",
    "sig2_y = np.array(sig2.actual_travel_time_n_target)\n",
    "sig2_y_zeromean = sig2_y - sig2_y.mean()\n",
    "\n",
    "plt.plot(sig1_X, sig1_y, sig2_X, sig2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sig1_X, sig1_y_zeromean, sig2_X, sig2_y_zeromean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "np.corrcoef(sig1_y_zeromean, sig2_y_zeromean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr_coef = pearsonr(sig1_y, sig2_y)\n",
    "pearsonr_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the entire network - all signals connected to node 726:\n",
    "# len(edge_series[726])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_series.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1283    218\n",
    "# 1498    218\n",
    "# 1497    212\n",
    "# 611     212\n",
    "# 622     212\n",
    "\n",
    "# %matplotlib\n",
    "sig1 = np.array(final_data_df[str(213)])\n",
    "\n",
    "list_of_node_crosscoef = []\n",
    "\n",
    "for i in edge_series[213]:\n",
    "    sig2 = np.array(final_data_df[str(i)])\n",
    "    list_of_node_crosscoef.append((i, np.corrcoef(sig1, sig2)[0][1]))\n",
    "\n",
    "zlst = list(zip(*list_of_node_crosscoef))\n",
    "list_of_node_crosscoef = pd.Series(zlst[1], index = zlst[0])\n",
    "list_of_node_crosscoef = list_of_node_crosscoef.dropna().sort_values()\n",
    "\n",
    "plt.scatter(list_of_node_crosscoef.index.values.tolist(), list_of_node_crosscoef.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation\n",
    "def autocorr(x, t=1):\n",
    "    return np.corrcoef(np.array([x[:-t], x[t:]]))\n",
    "autocorr(sig2_y_zeromean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib\n",
    "# import seaborn as sns\n",
    "# corr = final_data_df.corr()\n",
    "# sns.heatmap(corr, \n",
    "#             xticklabels=corr.columns.values,\n",
    "#             yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = final_data_df.corr().sort_values('213', ascending=False).index\n",
    "df_sorted = final_data_df.loc[:, ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how many sub components the graph has. This is an important step; make sure that the graph is connected, and there\n",
    "# every node is at least of degree 1.\n",
    "list(nx.connected_component_subgraphs(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [int(i) for i in final_data_df.columns.values.tolist()]\n",
    "\n",
    "adj_mat = nx.to_numpy_matrix(G, nodelist=a)\n",
    "\n",
    "np.savetxt(\"output_weights.csv\", adj_mat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_df.to_csv('output_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [str(i) for i in list(list(nx.connected_component_subgraphs(G))[0].nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = nx.to_numpy_matrix(G, nodelist=a)\n",
    "adj_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the graph another way! V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes = list(set(new_final.Route_Node_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_List_halt_ids = pd.Series(new_final.Route_Node_id_tuple.values,index=new_final.Route_Node_id).to_dict()\n",
    "len(Dict_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new graph:\n",
    "# Nodes = list(set(new_final.halt_id_x))\n",
    "# print(len(Nodes))\n",
    "\n",
    "# GPS is in Dict_List\n",
    "Edges = []\n",
    "for key, value in Dict_List_halt_ids.items():\n",
    "    for key_2, value_2 in Dict_List_halt_ids.items():\n",
    "        if key != key_2:\n",
    "#             if value[0] == value_2[0] or value[0] == value_2[1] or value[1] == value_2[0] or value[1] == value_2[1]: \n",
    "            if value[0] == value_2[0] or value[1] == value_2[1]:    \n",
    "                Edges.append((int(key), int(key_2), 1/(vincenty(Dict_List[int(key)], Dict_List[int(key_2)]).kilometers)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_G = nx.Graph()\n",
    "New_G.add_nodes_from(list_of_nodes_Road_toKeep)\n",
    "New_G.add_weighted_edges_from(Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(New_G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(New_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Line graph:\n",
    "# New_G_line = nx.line_graph(New_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(New_G_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(list(New_G_line.nodes())))\n",
    "# print(len(set(new_final.Route_Node_id)))\n",
    "\n",
    "# print(len(list(New_G_line.edges())))\n",
    "# # print(len(set(new_final.Route_Node_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(list(New_G.node))\n",
    "set2 = set(new_final.Route_Node_id)\n",
    "set2 - set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove nodes with no connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(New_G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Connected_New_G = max(nx.connected_component_subgraphs(New_G), key=len)\n",
    "nx.draw_networkx(Connected_New_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('final_10min.csv', nrows=252)\n",
    "\n",
    "df.interpolate(inplace=True, axis=1)\n",
    "\n",
    "print(df.isna().sum().sum())\n",
    "\n",
    "df_2 = pd.read_csv('gps_10min.csv')\n",
    "\n",
    "df = (df - df.mean()) / (df.std() ** 1.2)\n",
    "\n",
    "df_2.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Outer list:\n",
    "list_df_gps = df_2.columns.values.tolist()\n",
    "OuterList = []\n",
    "for index, row in df.iterrows():\n",
    "    Mid_List = []\n",
    "    for j in list_df_gps:\n",
    "        Time_List = df_2[j].values.tolist()\n",
    "#         print(type(j))\n",
    "#         print(row[0])\n",
    "        Time_List.append(row[j])\n",
    "        Mid_List.append(Time_List)\n",
    "    OuterList.append(Mid_List)\n",
    "\n",
    "m = folium.Map([47.36179377478453, 8.572671632662107], tiles='stamentoner', zoom_start=12)\n",
    "\n",
    "list_of_id = df_2.values\n",
    "list_of_id = list(zip(list_of_id[0], list_of_id[1]))\n",
    "\n",
    "for j in list_of_id:\n",
    "    folium.Circle(j, popup='<strong>Location One</strong>', radius=20).add_to(m)\n",
    "\n",
    "index = [str(timedelta(seconds=i * time_period + starting_time)) for i in set(final.intervals)] * int(df.shape[0]/len(set(final.intervals)))\n",
    "\n",
    "hm = plugins.HeatMapWithTime(OuterList, index=index)\n",
    "\n",
    "hm.add_to(m)\n",
    "\n",
    "m.save('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph with the heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about this visualization!\n",
    "\n",
    "There's a bug as of Oct. 2020 with folium's heatmapwithtime class. It's very well explained here:\n",
    "\n",
    "https://github.com/python-visualization/folium/issues/1221\n",
    "\n",
    "However, if the above was not available, simply replace:\n",
    "\n",
    "\"https://rawcdn.githack.com/socib/Leaflet.TimeDimension/master/dist/leaflet.timedimension.min.js\"\n",
    "\n",
    "in the index.html file (it is referenced as a script tag, you may opne index.html via sublime or any other text editor, and find it using ctrl+f) by:\n",
    "\n",
    "\"https://cdn.jsdelivr.net/npm/leaflet-timedimension@1.1.0/dist/leaflet.timedimension.min.js\"\n",
    "\n",
    "Be sure to save changes, and reload the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [int(i) for i in final_data_df.columns.values.tolist()]\n",
    "\n",
    "adj_mat = nx.to_numpy_matrix(G, nodelist=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_df.to_csv('output_v2_values.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"output_v2_weights.csv\", adj_mat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_G.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
